{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXlHoS7yokuo"
   },
   "source": [
    "## Resnet-18 Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AE0ue85okuo"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Resnet-18\n",
    "Weights are from https://www.kaggle.com/datasets/khoongweihao/pytorch-resnet18-34-50-101-152-pretrained?select=resnet18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18() \n",
    "\n",
    "weights_path = 'resnet18.pth'\n",
    "resnet18.load_state_dict(torch.load(weights_path)) #load pre-trained model weights\n",
    "\n",
    "resnet18.eval() #set model to evaluation mode\n",
    "print(resnet18) #verify model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysWIqOLXokuo"
   },
   "source": [
    "### Working with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_folder_gif() missing 1 required positional argument: 'fq2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflicker_img_whole\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_folder_gif\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract all convolutional layers\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mActivationModel\u001b[39;00m(Module):\n",
      "File \u001b[0;32m~/Desktop/bitirme/ssvep/flicker_img_whole.py:99\u001b[0m\n\u001b[1;32m     97\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m output_gif \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_animation2.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mcreate_folder_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfq1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gif\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_folder_gif() missing 1 required positional argument: 'fq2'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Module\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from flicker_img_whole import create_folder_gif\n",
    "\n",
    "# Extract all convolutional layers\n",
    "class ActivationModel(Module):\n",
    "    def __init__(self, model):\n",
    "        super(ActivationModel, self).__init__()\n",
    "        self.features = list(model.children())[:-2]  # Extract all layers except final FC layers\n",
    "        self.model = torch.nn.Sequential(*self.features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "            activations.append(x)\n",
    "        return activations\n",
    "\n",
    "# Preprocessing for ResNet-18\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), #for converting the PIL Image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\"\"\"\n",
    "#example usage:\n",
    "image_path = \"durov.jpg\"  # Ensure the path is correct\n",
    "frame = cv2.imread(image_path)\n",
    "img_array = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert OpenCV BGR to PIL RGB\n",
    "img = Image.fromarray(img_array)\n",
    "x = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _get_activations(model, frames):\n",
    "    activations = []\n",
    "    for frame in frames:\n",
    "        img_array = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img_array)\n",
    "        x = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            layer_activations = model(x)  # Forward pass through the model\n",
    "        activations.append(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def save_activations(activations, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for i, frame_activations in enumerate(activations):\n",
    "        for layer_idx, layer_activation in enumerate(frame_activations):\n",
    "            np.save(os.path.join(output_dir, f'frame_{i}_layer_{layer_idx}.npy'), layer_activation.cpu().numpy())\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# frames = flicker_img('durov.jpg', 5, 'output_animation_whole.gif')\n",
    "# activation_model = ActivationModel(resnet18)\n",
    "# activations = _get_activations(activation_model, frames)\n",
    "# save_activations(activations, 'activations_output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplot_activations(activations, \"frame_plots\", layer_number=4)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_activations(activations, output_dir, layer_number):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for frame_idx, frame_activations in enumerate(activations):\n",
    "        layer_activation = frame_activations[layer_number].cpu().numpy()\n",
    "        layer_activation = layer_activation.squeeze(0)  # Remove batch dimension (1, 64, 112, 112) -> (64, 112, 112)\n",
    "        ##plot the activation of entire filter.\n",
    "        #for filter_idx in range(layer_activation.shape[0]):\n",
    "        ##plot just 10 filters\n",
    "        for filter_idx in range(0):\n",
    "            # Create directories for layer and filter\n",
    "            layer_dir = os.path.join(output_dir, f'layer_{layer_number}')\n",
    "            filter_dir = os.path.join(layer_dir, f'filter_{filter_idx}')\n",
    "            if not os.path.exists(filter_dir):\n",
    "                os.makedirs(filter_dir)\n",
    "\n",
    "            plt.figure(figsize=(20, 20))\n",
    "            plt.title(f'Frame {frame_idx}, Layer {layer_number}, Filter {filter_idx}')\n",
    "            plt.imshow(layer_activation[filter_idx], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            save_path = os.path.join(filter_dir, f'frame_{frame_idx}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "plot_activations(activations, \"frame_plots\", layer_number=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is mean activation:\n",
    "def normalize_activations(activations, num_filters):\n",
    "    all_activations = np.array([layer_activation[:, :, :num_filters].mean(axis=(1, 2)) for layer_activation in activations])\n",
    "    mean_activations = all_activations.mean(axis=0)\n",
    "    std_activations = all_activations.std(axis=0)\n",
    "\n",
    "    normalized_activations = [(layer_activation[:, :, :num_filters] - mean_activations) for layer_activation in activations] #omitted division by std dev\n",
    "    return normalized_activations\n",
    "\"\"\"\n",
    "\"\"\" Normalize activations using median \n",
    "def normalize_activations(activations, num_filters):\n",
    "    \n",
    "    all_activations = np.array([np.median(layer_activation[:, :, :num_filters], axis=(1, 2)) for layer_activation in activations])\n",
    "    \n",
    "    median_activations = np.median(all_activations, axis=0)\n",
    "    \n",
    "    normalized_activations = [(layer_activation[:, :, :num_filters] - median_activations) for layer_activation in activations]\n",
    "\n",
    "    return normalized_activations\n",
    "\"\"\"\n",
    "\"\"\" Normalize using Max \"\"\"\n",
    "def normalize_activations(activations, num_filters):\n",
    "    all_activations = np.array([np.max(layer_activation[:, :, :num_filters], axis=(1, 2)) for layer_activation in activations])\n",
    "    \n",
    "    max_activations = np.max(all_activations, axis=0)\n",
    "    \n",
    "    normalized_activations = [(layer_activation[:, :, :num_filters] - max_activations) for layer_activation in activations]\n",
    "\n",
    "    return normalized_activations\n",
    "def normalize_activations_by_filter(activations):\n",
    "    filter_mean = activations.mean()\n",
    "    filter_std = activations.std()\n",
    "    \n",
    "    normalized_activations = (activations - filter_mean) #omitted division by std dev\n",
    "    return normalized_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_analysis(activations, output_dir, layer_number, num_filters=64, fps=30,cutoff=1):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    layer_activations = [frame_activations[layer_number].cpu().numpy() for frame_activations in activations]\n",
    "    num_frames = len(layer_activations)\n",
    "    \n",
    "    # Normalize activations\n",
    "    #layer_activations = normalize_activations(layer_activations, num_filters)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for filter_idx in range(num_filters):\n",
    "        #filter_activations = np.array([\n",
    "            #layer_activation[filter_idx].mean() for layer_activation in layer_activations\n",
    "        #])\n",
    "\n",
    "        filter_activations = np.array([\n",
    "            layer_activation.squeeze().mean()\n",
    "            for layer_activation in layer_activations\n",
    "        ])\n",
    "        \n",
    "        #apply highpass filter\n",
    "        filter_activations = highpass_filter(filter_activations, cutoff=cutoff, fs=fps)\n",
    "\n",
    "        # Normalize activations by filter\n",
    "        #filter_activations = normalize_activations(filter_activations,num_filters)\n",
    "\n",
    "        # Compute FFT\n",
    "        fft_result = np.fft.fft(filter_activations)\n",
    "        freqs = np.fft.fftfreq(num_frames, d=1/fps)\n",
    "        \n",
    "        # Plot frequency spectrum\n",
    "        plt.plot(freqs[0:num_frames // 2], np.abs(fft_result)[0:num_frames // 2], label=f'Filter {filter_idx}')\n",
    "    \n",
    "    plt.title(f'Frequency Analysis - Layer {layer_number}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    save_path = os.path.join(output_dir, f'layer_{layer_number}_frequency.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `activations` is already defined and contains the activations for each frame\n",
    "#requency_analysis(activations, \"frequency_analysis\", layer_number=4, num_filters=10, fps=30,cutoff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the functions we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 image files.\n",
      "Processing image: imgs/DSC_0545.JPG\n",
      "Image loaded: imgs/DSC_0545.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/DSC_0890.JPG\n",
      "Image loaded: imgs/DSC_0890.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/DSC_0895.JPG\n",
      "Image loaded: imgs/DSC_0895.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/DSC_0903.JPG\n",
      "Image loaded: imgs/DSC_0903.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/DSC_0909.JPG\n",
      "Image loaded: imgs/DSC_0909.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/ERHAN_HOCA.jpg\n",
      "Image loaded: imgs/ERHAN_HOCA.jpg\n",
      "Image dimensions: 418x494\n",
      "Number of frames generated: 30\n",
      "Processing image: imgs/cat_eye_sick.JPG\n",
      "Image loaded: imgs/cat_eye_sick.JPG\n",
      "Image dimensions: 4928x3264\n",
      "Number of frames generated: 30\n",
      "Final GIF saved: output_animation_whole.gif\n"
     ]
    }
   ],
   "source": [
    "# Create a new model that outputs the activations of all convolutional layers\n",
    "activation_model = ActivationModel(resnet18)\n",
    "\n",
    "# Get activations for each frame generated by flickering the image\n",
    "frames =create_folder_gif(\"imgs\", 5,6, 'output_animation_whole.gif' ,image_duration=1, fps=30, resize_dim=(640,480))\n",
    "activations = _get_activations(activation_model, frames)\n",
    "save_activations(activations, 'activations_output')\n",
    "\n",
    "#plot_activations(activations, \"frame_plots\", layer_number=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=1)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=2)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=3)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=4)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=5)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=6)\n",
    "frequency_analysis(activations, \"frequency_analysis_hsv_median\", layer_number=7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook2f55f2e572",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6302,
     "sourceId": 9896,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8782,
     "sourceId": 2431805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29980,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
