{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taqcv4_Ookul",
    "outputId": "10da88e2-712b-439a-f16d-fc48ca1db4f2"
   },
   "outputs": [],
   "source": [
    "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# # NOTEBOOK.\n",
    "# import kagglehub\n",
    "# crawford_vgg16_path = kagglehub.dataset_download('crawford/vgg16')\n",
    "# alxmamaev_flowers_recognition_path = kagglehub.dataset_download('alxmamaev/flowers-recognition')\n",
    "# print(alxmamaev_flowers_recognition_path)\n",
    "# print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2SV-UFbokum"
   },
   "source": [
    "# VGGNet Complete Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7JYkrM6o80_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLWQpbbmokum"
   },
   "source": [
    "### This notebook is a part of state-of-the-art Deep Learning Models Discussion.<br> This is third notebook in the series.<br>\n",
    "> Visit here: [SOTA Papers Discussion](https://www.kaggle.com/general/166349)<br>\n",
    "> Sample data used for demo: [Dataset](https://www.kaggle.com/alxmamaev/flowers-recognition)<br>\n",
    "> Read my other notebooks: [Notebooks](https://www.kaggle.com/blurredmachine/notebooks)<br>\n",
    "> SOTA Model Notebooks have been released earlier. [View previous notebooks](https://www.kaggle.com/general/166349)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOPVEEXwokun"
   },
   "source": [
    "![VGGNet](https://www.researchgate.net/publication/335353308/figure/fig2/AS:795069422522368@1566570730026/The-architecture-of-the-beamforming-deep-neural-network.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDD-NXJOokun"
   },
   "source": [
    "## Introduction on VGGNet\n",
    "The full name of VGG is the **Visual Geometry Group**, which belongs to the Department of Science and Engineering of Oxford University. It has released a series of convolutional network models beginning with VGG, which can be applied to face recognition and image classification, from VGG16 to VGG19. The original purpose of VGG's research on the depth of convolutional networks is to understand how the depth of convolutional networks affects the accuracy and accuracy of large-scale image classification and recognition. -Deep-16 CNN), in order to deepen the number of network layers and to avoid too many parameters, a small 3x3 convolution kernel is used in all layers.\n",
    "\n",
    "<a href=\"http://ethereon.github.io/netscope/#/gist/dc5003de6943ea5a6b8b\" target=\"_blank\">Network Structure of VGG19</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7-6jJRZokun"
   },
   "source": [
    "## The network structure\n",
    "- The input of VGG is set to an RGB image of 224x244 size. The average RGB value is calculated for all images on the training set image, and then the image is input as an input to the VGG convolution network. A 3x3 or 1x1 filter is used, and the convolution step is fixed. . There are 3 VGG fully connected layers, which can vary from VGG11 to VGG19 according to the total number of convolutional layers + fully connected layers. The minimum VGG11 has 8 convolutional layers and 3 fully connected layers. The maximum VGG19 has 16 convolutional layers. +3 fully connected layers. In addition, the VGG network is not followed by a pooling layer behind each convolutional layer, or a total of 5 pooling layers distributed under different convolutional layers. The following figure is VGG Structure diagram:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeCEAzteokun"
   },
   "source": [
    "![title](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/vgg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv0YtgP4okun"
   },
   "source": [
    "![config](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb1YK327okun"
   },
   "source": [
    "- VGG16 contains 16 layers and VGG19 contains 19 layers. A series of VGGs are exactly the same in the last three fully connected layers. The overall structure includes 5 sets of convolutional layers, followed by a MaxPool. The difference is that more and more cascaded convolutional layers are included in the five sets of convolutional layers .\n",
    "\n",
    "![config](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/config2.jpg)\n",
    "\n",
    "\n",
    "- Each convolutional layer in AlexNet contains only one convolution, and the size of the convolution kernel is 7 * 7 ,. In VGGNet, each convolution layer contains 2 to 4 convolution operations. The size of the convolution kernel is 3 * 3, the convolution step size is 1, the pooling kernel is 2 * 2, and the step size is 2. The most obvious improvement of VGGNet is to reduce the size of the convolution kernel and increase the number of convolution layers.\n",
    "\n",
    "\n",
    "![config](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/config3.jpg)\n",
    "\n",
    "\n",
    "- Using multiple convolution layers with smaller convolution kernels instead of a larger convolution layer with convolution kernels can reduce parameters on the one hand, and the author believes that it is equivalent to more non-linear mapping, which increases the Fit expression ability.\n",
    "\n",
    "![config](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/config4.png)\n",
    "\n",
    "- Two consecutive 3 * 3 convolutions are equivalent to a 5 * 5 receptive field, and three are equivalent to 7 * 7. The advantages of using three 3 * 3 convolutions instead of one 7 * 7 convolution are twofold : one, including three ReLu layers instead of one , makes the decision function more discriminative; and two, reducing parameters . For example, the input and output are all C channels. 3 convolutional layers using 3 * 3 require 3 (3 * 3 * C * C) = 27 * C * C, and 1 convolutional layer using 7 * 7 requires 7 * 7 * C * C = 49C * C. This can be seen as applying a kind of regularization to the 7 * 7 convolution, so that it is decomposed into three 3 * 3 convolutions.\n",
    "\n",
    "- The 1 * 1 convolution layer is mainly to increase the non-linearity of the decision function without affecting the receptive field of the convolution layer. Although the 1 * 1 convolution operation is linear, ReLu adds non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtWaPYmLokun"
   },
   "source": [
    "## Network Configuration\n",
    "\n",
    "- Table 1 shows all network configurations. These networks follow the same design principles, but differ in depth.\n",
    "\n",
    "![config](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/netconvgg.png)\n",
    "\n",
    "- This picture is definitely used when introducing VGG16. This picture contains a lot of information. My interpretation here may be limited. If you have any supplements, please leave a message.\n",
    "\n",
    "* **Number 1** : This is a comparison chart of 6 networks. From A to E, the network is getting deeper. Several layers have been added to verify the effect.\n",
    "\n",
    "* **Number 2** : Each column explains the structure of each network in detail.\n",
    "\n",
    "* **Number 3**: This is a correct way to do experiments, that is, use the simplest method to solve the problem , and then gradually optimize for the problems that occur.\n",
    "\n",
    "**Network A**: First mention a shallow network, this network can easily converge on ImageNet.\n",
    "And then?<br>\n",
    "**Network A-LRN**: Add something that someone else (AlexNet) has experimented to say is effective (LRN), but it seems useless.\n",
    "And then?<br>\n",
    "**Network B**: Then try adding 2 layers? Seems to be effective.\n",
    "And then?<br>\n",
    "**Network C**: Add two more layers of 1 * 1 convolution, and it will definitely converge. The effect seems to be better. A little excited.\n",
    "And then?<br>\n",
    "**Network D**: Change the 1 * 1 convolution kernel to 3 * 3. Try it. The effect has improved again. Seems to be the best (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3x6EYhRokuo"
   },
   "source": [
    "## Training\n",
    "\n",
    "- **The optimization method** is a stochastic gradient descent SGD + momentum (0.9) with momentum.\n",
    "The batch size is 256.\n",
    "\n",
    "- **Regularization** : L2 regularization is used, and the weight decay is 5e-4. Dropout is after the first two fully connected layers, p = 0.5.\n",
    "\n",
    "- Although it is deeper and has more parameters than the AlexNet network, we speculate that VGGNet can converge in less cycles for two reasons: one, the greater depth and smaller convolutions bring implicit regularization ; Second, some layers of pre-training.\n",
    "\n",
    "- **Parameter initialization** : For a shallow A network, parameters are randomly initialized, the weight w is sampled from N (0, 0.01), and the bias is initialized to 0. Then, for deeper networks, first the first four convolutional layers and three fully connected layers are initialized with the parameters of the A network. However, it was later discovered that it is also possible to directly initialize it without using pre-trained parameters.\n",
    "\n",
    "- In order to obtain a 224 * 224 input image, each rescaled image is randomly cropped in each SGD iteration. In order to enhance the data set, the cropped image is also randomly flipped horizontally and RGB color shifted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRqD_rGZokuo"
   },
   "source": [
    "## Summary of VGGNet improvement points\n",
    "\n",
    "1. A smaller 3 * 3 convolution kernel and a deeper network are used . The stack of two 3 * 3 convolution kernels is relative to the field of view of a 5 * 5 convolution kernel, and the stack of three 3 * 3 convolution kernels is equivalent to the field of view of a 7 * 7 convolution kernel. In this way, there can be fewer parameters (3 stacked 3 * 3 structures have only 7 * 7 structural parameters (3 * 3 * 3) / (7 * 7) = 55%); on the other hand, they have more The non-linear transformation increases the ability of CNN to learn features.\n",
    "\n",
    "2. In the convolutional structure of VGGNet, a 1 * 1 convolution kernel is introduced. Without affecting the input and output dimensions, non-linear transformation is introduced to increase the expressive power of the network and reduce the amount of calculation.\n",
    "\n",
    "3. During training, first train a simple (low-level) VGGNet A-level network, and then use the weights of the A network to initialize the complex models that follow to speed up the convergence of training .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6GO-mk2okuo"
   },
   "source": [
    "## Some basic FAQs\n",
    "\n",
    "**Q1: Why can 3 3x3 convolutions replace 7x7 convolutions?**\n",
    "\n",
    "***Answer 1***<br>\n",
    "3 3x3 convolutions, using 3 non-linear activation functions, increasing non-linear expression capabilities, making the segmentation plane more separable\n",
    "Reduce the number of parameters. For the convolution kernel of C channels, 7x7 contains parameters , and the number of 3 3x3 parameters is greatly reduced.\n",
    "\n",
    "\n",
    "**Q2: The role of 1x1 convolution kernel**\n",
    "\n",
    "***Answer 2***<br>\n",
    "Increase the nonlinearity of the model without affecting the receptive field\n",
    "1x1 winding machine is equivalent to linear transformation, and the non-linear activation function plays a non-linear role\n",
    "\n",
    "\n",
    "**Q3: The effect of network depth on results (in the same year, Google also independently released the network GoogleNet with a depth of 22 layers)**\n",
    "\n",
    "***Answer 3***<br>\n",
    "VGG and GoogleNet models are deep\n",
    "Small convolution\n",
    "VGG only uses 3x3, while GoogleNet uses 1x1, 3x3, 5x5, the model is more complicated (the model began to use a large convolution kernel to reduce the calculation of the subsequent machine layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXlHoS7yokuo"
   },
   "source": [
    "## VGG-16 Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AE0ue85okuo"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "lOuj_Uejokuo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p05YT6Yuokuo"
   },
   "source": [
    "### Defining input image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "zyH4QlIeokuo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_input = Input((224,224,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AELE34Gaokuo"
   },
   "source": [
    "### Building a VGG-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "W3mQEcKmokuo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n",
    "conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "pool1  = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "pool2  = MaxPooling2D((2, 2))(conv4)\n",
    "\n",
    "conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "pool3  = MaxPooling2D((2, 2))(conv7)\n",
    "\n",
    "conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "pool4  = MaxPooling2D((2, 2))(conv10)\n",
    "\n",
    "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
    "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
    "pool5  = MaxPooling2D((2, 2))(conv13)\n",
    "\n",
    "flat   = Flatten()(pool5)\n",
    "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
    "dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
    "output = Dense(1000, activation=\"softmax\")(dense2)\n",
    "\n",
    "vgg16_model  = Model(inputs=_input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysWIqOLXokuo"
   },
   "source": [
    "### Working with pretrained model\n",
    "\n",
    "- Keras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers.\n",
    "\n",
    "- Before using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded: s.jpeg\n",
      "Image dimensions: 1000x711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m             np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_layer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), layer_activation)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mflicker_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms.jpeg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_animation_whole.gif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m activations \u001b[38;5;241m=\u001b[39m _get_activations(activation_model, frames)\n\u001b[1;32m     40\u001b[0m save_activations(activations, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations_output\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/grad project/code/ssvep/flicker_img_whole.py:25\u001b[0m, in \u001b[0;36mflicker_img\u001b[0;34m(img_path, fq1, output_path, duration, fps, resize_dim)\u001b[0m\n\u001b[1;32m     22\u001b[0m brightness_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.75\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m fq1 \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m/\u001b[39m fps)  \u001b[38;5;66;03m# Range: 0.5 to 1.0\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply brightness changes\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbrightness_factor\u001b[49m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     29\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;66;03m#convert to RGB\u001b[39;00m\n\u001b[1;32m     30\u001b[0m frame_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame_rgb, resize_dim) \u001b[38;5;66;03m#resize frame\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16, decode_predictions, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from flicker_img_whole import flicker_img\n",
    "# Load the VGG16 model\n",
    "vgg16_weights = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "vgg16_model = VGG16(weights=vgg16_weights)\n",
    "\n",
    "# Create a new model that outputs the activations of all convolutional layers\n",
    "layer_outputs = [layer.output for layer in vgg16_model.layers if 'conv' in layer.name]\n",
    "activation_model = Model(inputs=vgg16_model.input, outputs=layer_outputs)\n",
    "\n",
    "def _get_activations(model, frames):\n",
    "    activations = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        img_array = cv2.resize(frame, (224, 224))  # Resize to VGG16 input size\n",
    "        x = np.expand_dims(img_array, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        layer_activations = model.predict(x)\n",
    "        activations.append(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def save_activations(activations, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for i, frame_activations in enumerate(activations):\n",
    "        for layer_idx, layer_activation in enumerate(frame_activations):\n",
    "            np.save(os.path.join(output_dir, f'frame_{i}_layer_{layer_idx}.npy'), layer_activation)\n",
    "\n",
    "# Example usage\n",
    "frames = flicker_img('s.jpeg', 5, 'output_animation_whole.gif')\n",
    "activations = _get_activations(activation_model, frames)\n",
    "save_activations(activations, 'activations_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m             plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mplot_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe_plots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 25\u001b[0m, in \u001b[0;36mplot_activations\u001b[0;34m(activations, output_dir, layer_number)\u001b[0m\n\u001b[1;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(filter_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/pyplot.py:1229\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[0;32m-> 1229\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/backend_bases.py:1905\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 1905\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/figure.py:3162\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3162\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3165\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/axes/_base.py:3137\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3135\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3137\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:653\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    651\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     im, l, b, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_magnification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:952\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    949\u001b[0m transformed_bbox \u001b[38;5;241m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    950\u001b[0m clip \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_on()\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mbbox)\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmagnification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsampled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:526\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    521\u001b[0m mask \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mwhere(A\u001b[38;5;241m.\u001b[39mmask, np\u001b[38;5;241m.\u001b[39mfloat32(np\u001b[38;5;241m.\u001b[39mnan), np\u001b[38;5;241m.\u001b[39mfloat32(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m A\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# nontrivial mask\u001b[39;00m\n\u001b[1;32m    523\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones_like(A, np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# we always have to interpolate the mask to account for\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# non-affine transformations\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m out_alpha \u001b[38;5;241m=\u001b[39m \u001b[43m_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mask  \u001b[38;5;66;03m# Make sure we don't use mask anymore!\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# Agg updates out_alpha in place.  If the pixel has no image\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# data it will not be updated (and still be 0 as we initialized\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# it), if input data that would go into that output pixel than\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# it will be `nan`, if all the input data for a pixel is good\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# it will be 1, and if there is _some_ good data in that output\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# pixel it will be between [0, 1] (such as a rotated image).\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/matplotlib/image.py:208\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     resample \u001b[38;5;241m=\u001b[39m image_obj\u001b[38;5;241m.\u001b[39mget_resample()\n\u001b[0;32m--> 208\u001b[0m \u001b[43m_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_interpd_\u001b[49m\u001b[43m[\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filternorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filterrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_activations(activations, output_dir, layer_number):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for frame_idx, frame_activations in enumerate(activations):\n",
    "        layer_activation = frame_activations[layer_number]\n",
    "        # Get the number of filters in the layer\n",
    "        num_filters_in_layer = layer_activation.shape[-1]\n",
    "        # Plot the first `num_filters` filters\n",
    "        for filter_idx in range( 10):\n",
    "            # Create directories for layer and filter\n",
    "            layer_dir = os.path.join(output_dir, f'layer_{layer_number}')\n",
    "            filter_dir = os.path.join(layer_dir, f'filter_{filter_idx}')\n",
    "            if not os.path.exists(filter_dir):\n",
    "                os.makedirs(filter_dir)\n",
    "            \n",
    "            plt.figure(figsize=(20, 20))\n",
    "            plt.title(f'Frame {frame_idx}, Layer {layer_number}, Filter {filter_idx}')\n",
    "            plt.imshow(layer_activation[0, :, :, filter_idx], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            save_path = os.path.join(filter_dir, f'frame_{frame_idx}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "\n",
    "# Example usage\n",
    "plot_activations(activations, \"frame_plots\", layer_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import detrend,butter,filtfilt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def frequency_analysis(activations, output_dir, layer_number, num_filters=10, fps=30,cutoff=1):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    layer_activations = [frame_activations[layer_number] for frame_activations in activations]\n",
    "    num_frames = len(layer_activations)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for filter_idx in range(num_filters):\n",
    "        filter_activations = [layer_activation[0, :, :, filter_idx].mean() for layer_activation in layer_activations]\n",
    "        filter_activations = np.array(filter_activations)\n",
    "        #apply highpass filter\n",
    "        filter_activations = highpass_filter(filter_activations, cutoff=cutoff, fs=fps)\n",
    "        # Compute FFT\n",
    "        fft_result = np.fft.fft(filter_activations)\n",
    "        freqs = np.fft.fftfreq(num_frames, d=1/fps)\n",
    "        \n",
    "        # Plot frequency spectrum\n",
    "        plt.plot(freqs[0:num_frames // 2], np.abs(fft_result)[0:num_frames // 2], label=f'Filter {filter_idx}')\n",
    "    \n",
    "    plt.title(f'Frequency Analysis - Layer {layer_number}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    save_path = os.path.join(output_dir, f'layer_{layer_number}_frequency.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `activations` is already defined and contains the activations for each frame\n",
    "frequency_analysis(activations, \"frequency_analysis\", layer_number=4, num_filters=10, fps=30,cutoff=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook2f55f2e572",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6302,
     "sourceId": 9896,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8782,
     "sourceId": 2431805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29980,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
