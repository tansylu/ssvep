{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXlHoS7yokuo"
   },
   "source": [
    "## Resnet-18 Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AE0ue85okuo"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Resnet-18\n",
    "Weights are from https://www.kaggle.com/datasets/khoongweihao/pytorch-resnet18-34-50-101-152-pretrained?select=resnet18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18() \n",
    "\n",
    "weights_path = 'resnet18.pth'\n",
    "resnet18.load_state_dict(torch.load(weights_path)) #load pre-trained model weights\n",
    "\n",
    "resnet18.eval() #set model to evaluation mode\n",
    "print(resnet18) #verify model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysWIqOLXokuo"
   },
   "source": [
    "### Working with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nframes = flicker_img('s.jpeg', 5, 'output_animation_whole.gif')\\nactivations = _get_activations(activation_model, frames)\\nsave_activations(activations, 'activations_output')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Module\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from flicker_img_whole import flicker_img\n",
    "\n",
    "# Extract all convolutional layers\n",
    "class ActivationModel(Module):\n",
    "    def __init__(self, model):\n",
    "        super(ActivationModel, self).__init__()\n",
    "        self.features = list(model.children())[:-2]  # Extract all layers except final FC layers\n",
    "        self.model = torch.nn.Sequential(*self.features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "            activations.append(x)\n",
    "        return activations\n",
    "\n",
    "# Preprocessing for ResNet-18\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), #for converting the PIL Image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#example usage:\n",
    "'''\n",
    "img_array = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert OpenCV BGR to PIL RGB\n",
    "img = Image.fromarray(img_array)\n",
    "x = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "'''\n",
    "\n",
    "\n",
    "def _get_activations(model, frames):\n",
    "    activations = []\n",
    "    for frame in frames:\n",
    "        img_array = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR (cv2) to RGB (PIL)\n",
    "        img = Image.fromarray(img_array)\n",
    "        x = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            layer_activations = model(x)  # Forward pass through the model\n",
    "        activations.append(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def save_activations(activations, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for i, frame_activations in enumerate(activations):\n",
    "        for layer_idx, layer_activation in enumerate(frame_activations):\n",
    "            np.save(os.path.join(output_dir, f'frame_{i}_layer_{layer_idx}.npy'), layer_activation.cpu().numpy())\n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "frames = flicker_img('s.jpeg', 5, 'output_animation_whole.gif')\n",
    "activations = _get_activations(activation_model, frames)\n",
    "save_activations(activations, 'activations_output')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_activations(activations, output_dir, layer_number):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for frame_idx, frame_activations in enumerate(activations):\n",
    "        layer_activation = frame_activations[layer_number].cpu().numpy()\n",
    "        layer_activation = layer_activation.squeeze(0)  # Remove batch dimension (1, 64, 112, 112) -> (64, 112, 112)\n",
    "\n",
    "        for filter_idx in range(layer_activation.shape[0]):\n",
    "            # Create directories for layer and filter\n",
    "            layer_dir = os.path.join(output_dir, f'layer_{layer_number}')\n",
    "            filter_dir = os.path.join(layer_dir, f'filter_{filter_idx}')\n",
    "            if not os.path.exists(filter_dir):\n",
    "                os.makedirs(filter_dir)\n",
    "\n",
    "            plt.figure(figsize=(20, 20))\n",
    "            plt.title(f'Frame {frame_idx}, Layer {layer_number}, Filter {filter_idx}')\n",
    "            plt.imshow(layer_activation[filter_idx], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            save_path = os.path.join(filter_dir, f'frame_{frame_idx}.png')\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "\n",
    "# Example usage\n",
    "#plot_activations(activations, \"frame_plots\", layer_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_activations(activations, num_filters):\n",
    "    all_activations = np.array([layer_activation[:, :, :num_filters].mean(axis=(1, 2)) for layer_activation in activations])\n",
    "    mean_activations = all_activations.mean(axis=0)\n",
    "    std_activations = all_activations.std(axis=0)\n",
    "\n",
    "    normalized_activations = [(layer_activation[:, :, :num_filters] - mean_activations) for layer_activation in activations] #omitted division by std dev\n",
    "    return normalized_activations\n",
    "\n",
    "def normalize_activations_by_filter(activations):\n",
    "    filter_mean = activations.mean()\n",
    "    filter_std = activations.std()\n",
    "    \n",
    "    normalized_activations = (activations - filter_mean) #omitted division by std dev\n",
    "    return normalized_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_analysis(activations, output_dir, layer_number, num_filters=10, fps=30,cutoff=1):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    layer_activations = [frame_activations[layer_number].cpu().numpy() for frame_activations in activations]\n",
    "    num_frames = len(layer_activations)\n",
    "    \n",
    "    # Normalize activations\n",
    "    #layer_activations = normalize_activations(layer_activations, num_filters)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for filter_idx in range(num_filters):\n",
    "        #filter_activations = np.array([\n",
    "            #layer_activation[filter_idx].mean() for layer_activation in layer_activations\n",
    "        #])\n",
    "\n",
    "        filter_activations = np.array([\n",
    "            layer_activation.squeeze().mean() \n",
    "            for layer_activation in layer_activations\n",
    "        ])\n",
    "        \n",
    "        #apply highpass filter\n",
    "        filter_activations = highpass_filter(filter_activations, cutoff=cutoff, fs=fps)\n",
    "\n",
    "        # Normalize activations by filter\n",
    "        filter_activations = normalize_activations_by_filter(filter_activations)\n",
    "\n",
    "        # Compute FFT\n",
    "        fft_result = np.fft.fft(filter_activations)\n",
    "        freqs = np.fft.fftfreq(num_frames, d=1/fps)\n",
    "        \n",
    "        # Plot frequency spectrum\n",
    "        plt.plot(freqs[0:num_frames // 2], np.abs(fft_result)[0:num_frames // 2], label=f'Filter {filter_idx}')\n",
    "    \n",
    "    plt.title(f'Frequency Analysis - Layer {layer_number}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    save_path = os.path.join(output_dir, f'layer_{layer_number}_frequency.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `activations` is already defined and contains the activations for each frame\n",
    "#requency_analysis(activations, \"frequency_analysis\", layer_number=4, num_filters=10, fps=30,cutoff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the functions we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model that outputs the activations of all convolutional layers\n",
    "activation_model = ActivationModel(resnet18)\n",
    "\n",
    "# Get activations for each frame generated by flickering the image\n",
    "frames = flicker_img('butterfly.jpg', 5, 'output_animation_whole.gif')\n",
    "\n",
    "activations = _get_activations(activation_model, frames)\n",
    "save_activations(activations, 'activations_output')\n",
    "\n",
    "plot_activations(activations, \"frame_plots\", layer_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_analysis(activations, \"frequency_analysis\", layer_number=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook2f55f2e572",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6302,
     "sourceId": 9896,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8782,
     "sourceId": 2431805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29980,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
